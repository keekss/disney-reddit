{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pmaw import PushshiftAPI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download dataset\n",
    "### Pulls pushshift data using PMAW api, visit https://github.com/mattpodolak/pmaw for docs and details\n",
    "### Running will make pull request, unneccessary and takes forever so only uncomment if need to make a new dataset. Work from csv instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of pulling a specified number of posts (aka submissions) from a subreddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = PushshiftAPI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:pmaw.PushshiftAPIBase:Not all PushShift shards are active. Query results may be incomplete.\n",
      "INFO:pmaw.PushshiftAPIBase:Total:: Success Rate: 100.00% - Requests: 20 - Batches: 2 - Items Remaining: 0\n"
     ]
    }
   ],
   "source": [
    "# Replace value for subreddit with desired subreddit name, case-sensitive\n",
    "# Replace value for limit to set desired number of submissions(posts) to pull\n",
    "# limit=None will pull all submissions(posts) from the subreddit\n",
    "# Beware, pulling all posts will take time, mem, cpu\n",
    "submissions = api.search_submissions(subreddit=\"disney\", limit=2000)\n",
    "\n",
    "sub_df = pd.DataFrame(submissions)\n",
    "\n",
    "# Rename path, leaving the rest of the parameters should be fine\n",
    "# sub_df.to_csv('./data/disney_500_subs.csv', header=True, index=False, columns=list(sub_df.axes[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of pulling all posts (aka submissions) from a subreddit\n",
    "##### Beware, pulling all posts will take time, mem, cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:pmaw.PushshiftAPIBase:Not all PushShift shards are active. Query results may be incomplete.\n",
      "INFO:pmaw.PushshiftAPIBase:27963 result(s) available in Pushshift\n",
      "INFO:pmaw.PushshiftAPIBase:Checkpoint:: Success Rate: 100.00% - Requests: 100 - Batches: 10 - Items Remaining: 18412\n",
      "INFO:pmaw.PushshiftAPIBase:Total:: Success Rate: 100.00% - Requests: 133 - Batches: 14 - Items Remaining: 16980\n",
      "WARNING:pmaw.PushshiftAPIBase:Not all PushShift shards are active. Query results may be incomplete.\n",
      "INFO:pmaw.PushshiftAPIBase:10 result(s) not found in Pushshift\n",
      "WARNING:pmaw.PushshiftAPIBase:Not all PushShift shards are active. Query results may be incomplete.\n",
      "WARNING:pmaw.PushshiftAPIBase:Not all PushShift shards are active. Query results may be incomplete.\n",
      "WARNING:pmaw.PushshiftAPIBase:Not all PushShift shards are active. Query results may be incomplete.\n",
      "WARNING:pmaw.PushshiftAPIBase:Not all PushShift shards are active. Query results may be incomplete.\n",
      "WARNING:pmaw.PushshiftAPIBase:Not all PushShift shards are active. Query results may be incomplete.\n",
      "INFO:pmaw.PushshiftAPIBase:Checkpoint:: Success Rate: 100.00% - Requests: 193 - Batches: 20 - Items Remaining: 10981\n",
      "WARNING:pmaw.PushshiftAPIBase:Not all PushShift shards are active. Query results may be incomplete.\n",
      "WARNING:pmaw.PushshiftAPIBase:Not all PushShift shards are active. Query results may be incomplete.\n",
      "WARNING:pmaw.PushshiftAPIBase:Not all PushShift shards are active. Query results may be incomplete.\n",
      "WARNING:pmaw.PushshiftAPIBase:Not all PushShift shards are active. Query results may be incomplete.\n",
      "WARNING:pmaw.PushshiftAPIBase:Not all PushShift shards are active. Query results may be incomplete.\n",
      "INFO:pmaw.PushshiftAPIBase:Total:: Success Rate: 100.00% - Requests: 258 - Batches: 27 - Items Remaining: 6608\n",
      "WARNING:pmaw.PushshiftAPIBase:Not all PushShift shards are active. Query results may be incomplete.\n",
      "INFO:pmaw.PushshiftAPIBase:4 result(s) not found in Pushshift\n",
      "WARNING:pmaw.PushshiftAPIBase:Not all PushShift shards are active. Query results may be incomplete.\n",
      "WARNING:pmaw.PushshiftAPIBase:Not all PushShift shards are active. Query results may be incomplete.\n",
      "WARNING:pmaw.PushshiftAPIBase:Not all PushShift shards are active. Query results may be incomplete.\n",
      "INFO:pmaw.PushshiftAPIBase:Checkpoint:: Success Rate: 100.00% - Requests: 282 - Batches: 30 - Items Remaining: 4755\n",
      "WARNING:pmaw.PushshiftAPIBase:Not all PushShift shards are active. Query results may be incomplete.\n",
      "WARNING:pmaw.PushshiftAPIBase:Not all PushShift shards are active. Query results may be incomplete.\n",
      "WARNING:pmaw.PushshiftAPIBase:Not all PushShift shards are active. Query results may be incomplete.\n",
      "WARNING:pmaw.PushshiftAPIBase:Not all PushShift shards are active. Query results may be incomplete.\n",
      "WARNING:pmaw.PushshiftAPIBase:Not all PushShift shards are active. Query results may be incomplete.\n",
      "WARNING:pmaw.PushshiftAPIBase:Not all PushShift shards are active. Query results may be incomplete.\n",
      "WARNING:pmaw.PushshiftAPIBase:Not all PushShift shards are active. Query results may be incomplete.\n",
      "INFO:pmaw.PushshiftAPIBase:Total:: Success Rate: 100.00% - Requests: 340 - Batches: 37 - Items Remaining: 13\n",
      "WARNING:pmaw.PushshiftAPIBase:Not all PushShift shards are active. Query results may be incomplete.\n",
      "INFO:pmaw.PushshiftAPIBase:3 result(s) not found in Pushshift\n",
      "WARNING:pmaw.PushshiftAPIBase:Not all PushShift shards are active. Query results may be incomplete.\n",
      "INFO:pmaw.PushshiftAPIBase:Total:: Success Rate: 100.00% - Requests: 350 - Batches: 38 - Items Remaining: 0\n"
     ]
    }
   ],
   "source": [
    "all_submissions = api.search_submissions(subreddit=\"DisneyPlus\", limit=None)\n",
    "\n",
    "all_sub_df = pd.DataFrame(all_submissions)\n",
    "\n",
    "# Rename path, leave the rest of the parameters\n",
    "all_sub_df.to_csv('./data/disneyplus_all_subs.csv', header=True, index=False, columns=list(all_sub_df.axes[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments\n",
    "\n",
    "### Download comments based on submissions pulled in above query. \n",
    "#### This code block will only the comments of the submissions queried above and saves to a csv. It takes much longer than simply pulling submissions\n",
    "\n",
    "###### Running will make pull request, unneccessary and takes forever so only uncomment if need to make a new dataset. Work from csv instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace path\n",
    "subs_df = pd.read_csv('./data/disney_500_subs.csv',header=0) \n",
    "\n",
    "sub_ids = list(subs_df.loc[:, 'id']) \n",
    "\n",
    "# retrieve comment ids for submissions\n",
    "comment_ids = api.search_submission_comment_ids(ids=sub_ids)\n",
    "comment_ids = list(comment_ids)\n",
    "\n",
    "# retrieve comments by id\n",
    "comments = api.search_comments(ids=comment_ids)\n",
    "\n",
    "comments_df = pd.DataFrame(comments)\n",
    "\n",
    "# Replace path\n",
    "comments_df.to_csv('./data/disney_500_comments.csv', header=True, index=False, columns=list(comments_df.axes[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to pull a given number of comments rather than just those from samples already queried (FASTER)\n",
    "#### This method is much faster and should be used for pulling a large number of comments or all comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = PushshiftAPI()\n",
    "\n",
    "# Replace value for subreddit with desired subreddit name, case-sensitive\n",
    "comments = api.search_comments(subreddit=\"wallstreetbets\", limit=300000)\n",
    "\n",
    "comments_df = pd.DataFrame(comments)\n",
    "\n",
    "# Replace path\n",
    "comments_df.to_csv('./data/wallstreetbets_comments.csv', header=True, index=False, columns=list(comments_df.axes[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with submissions data, mainly adding a new date column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sub_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-66bca1fb78df>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Returns column of dates when submissions were created\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0msub_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreated_utc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'sub_df' is not defined"
     ]
    }
   ],
   "source": [
    "# Returns column of dates when submissions were created\n",
    "# Times are UTC timezone and Unix in format\n",
    "all_sub_df.created_utc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single example of converting from Unix date-time to readable string date-time format\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "ts = sub_df.created_utc[0]\n",
    "print(ts)\n",
    "print(datetime.utcfromtimestamp(ts).strftime('%m-%d-%Y'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new list of all Unix times in readable string date-time format\n",
    "\n",
    "sub_dates = []\n",
    "\n",
    "for _ in sub_df['created_utc']:\n",
    "    fts = datetime.utcfromtimestamp(_).strftime('%m-%d-%Y')\n",
    "    sub_dates.append(fts)\n",
    "\n",
    "# Appends new column to dataframe, contains readable string date-times\n",
    "sub_df['creation_date'] = all_sub_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saves dataset to csv\n",
    "# Replace path\n",
    "sub_df.to_csv('./data/disney_all_subs_w_dates.csv', header=True, index=False, columns=list(sub_df.axes[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with comments data, mainly adding a new dates column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "com_dates = []\n",
    "\n",
    "for _ in comments_df['created_utc']:\n",
    "    fts = datetime.utcfromtimestamp(_).strftime('%m-%d-%Y')\n",
    "    com_dates.append(fts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_df['creation_date'] = com_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
